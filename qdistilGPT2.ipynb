{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3YbmPU438Qj0KFgbtpc6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aang-CHO2/gradio_distilGPT2/blob/main/qdistilGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries in Google Colab\n",
        "!pip install transformers gradio pdfplumber torch --quiet\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pdfplumber  # Optimized PDF processing\n",
        "import os\n",
        "\n",
        "# Use a smaller model like DistilGPT-2 or GPT-2\n",
        "model_name = \"distilgpt2\"  # You can also use \"gpt2\" or \"flan-t5-small\"\n",
        "\n",
        "# Use GPU if available, else fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Cache for tokenized inputs (to avoid re-tokenizing the same content)\n",
        "token_cache = {}\n",
        "\n",
        "# Function to extract text from the PDF using pdfplumber for faster processing\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() if page.extract_text() else ''\n",
        "    return text.strip() if text else \"Error: Could not extract text from PDF.\"\n",
        "\n",
        "# Function to extract text from a .txt file\n",
        "def extract_text_from_txt(txt_file):\n",
        "    try:\n",
        "        text = txt_file.read().decode('utf-8')\n",
        "    except Exception as e:\n",
        "        return f\"Error: Could not read the text file. Details: {str(e)}\"\n",
        "\n",
        "    return text.strip() if text else \"Error: Text file is empty.\"\n",
        "\n",
        "# Load the file and extract its content based on the file content\n",
        "def extract_text_from_file(file_obj):\n",
        "    # Check if file_obj is None (no file uploaded)\n",
        "    if file_obj is None:\n",
        "        return \"Error: No file uploaded. Please upload a PDF or TXT file.\"\n",
        "\n",
        "    # Check if the file is a PDF or TXT file based on content type\n",
        "    file_name = file_obj.name if hasattr(file_obj, 'name') else \"unknown\"\n",
        "\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(file_obj)\n",
        "    elif file_name.endswith(\".txt\"):\n",
        "        return extract_text_from_txt(file_obj)\n",
        "    else:\n",
        "        return \"Error: Unsupported file format. Please provide a .pdf or .txt file.\"\n",
        "\n",
        "# Generate prompt suggestions based on content\n",
        "def generate_prompt_suggestions(file_text):\n",
        "    # Analyze the content to suggest some starter prompts (this is a basic implementation)\n",
        "    if \"chapter\" in file_text.lower() or \"introduction\" in file_text.lower():\n",
        "        return [\"Can you summarize this chapter?\", \"What is the key takeaway from the introduction?\", \"Explain the main argument.\"]\n",
        "    elif \"conclusion\" in file_text.lower() or \"results\" in file_text.lower():\n",
        "        return [\"What are the results mentioned in the document?\", \"Can you summarize the conclusion?\", \"What are the key findings?\"]\n",
        "    else:\n",
        "        return [\"What is this document about?\", \"Summarize the content of this file.\", \"Can you explain the key points?\"]\n",
        "\n",
        "# Define the chatbot function\n",
        "def chatbot_fn(prompt, file_obj, chatbot_history=None, max_new_tokens=150):\n",
        "    try:\n",
        "        # Initialize chatbot_history if it's None\n",
        "        if chatbot_history is None:\n",
        "            chatbot_history = []\n",
        "\n",
        "        # Extract text from the file\n",
        "        file_text = extract_text_from_file(file_obj)\n",
        "\n",
        "        if file_text.startswith(\"Error\"):\n",
        "            return file_text, chatbot_history  # Return the error message\n",
        "\n",
        "        # Cache tokenized inputs to avoid redundant work (faster tokenization)\n",
        "        if file_text not in token_cache:\n",
        "            token_cache[file_text] = tokenizer(file_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "        inputs = token_cache[file_text]  # Retrieve tokenized input from cache\n",
        "\n",
        "        # Generate response with `max_new_tokens`\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # Decode the response\n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Check if the \"Assistant:\" label exists in the response\n",
        "        if \"Assistant:\" in response_text:\n",
        "            assistant_response = response_text.split(\"Assistant:\")[-1].strip()\n",
        "        else:\n",
        "            assistant_response = response_text if response_text else \"I couldn't generate a proper response.\"\n",
        "\n",
        "        # Update chatbot history with the user and assistant responses\n",
        "        chatbot_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "        chatbot_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        return assistant_response, chatbot_history\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", chatbot_history\n",
        "\n",
        "# Simplified Gradio Interface (light mode, no custom CSS)\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"<h1>Document-based Chatbot (Light Mode)</h1>\")\n",
        "\n",
        "    # Chat window\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            chatbox = gr.HTML(elem_id=\"chatbox\", value=\"\")\n",
        "\n",
        "    # Inputs\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            user_prompt = gr.Textbox(label=\"Your message\")\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(label=\"Upload PDF or TXT file\")\n",
        "\n",
        "    # Slider for Max New Tokens\n",
        "    max_new_tokens = gr.Slider(50, 300, step=10, label=\"Max New Tokens\")\n",
        "\n",
        "    # Suggested prompts area\n",
        "    with gr.Row():\n",
        "        suggested_prompts = gr.HTML(\"\")\n",
        "\n",
        "    # Output area (chat history)\n",
        "    output_text = gr.Textbox(visible=False)\n",
        "\n",
        "    # Chat history state\n",
        "    chatbot_history = gr.State([])\n",
        "\n",
        "    # Button to submit\n",
        "    submit_button = gr.Button(\"Send\")\n",
        "\n",
        "    # Function to generate and display suggested prompts\n",
        "    def update_suggested_prompts(file_obj):\n",
        "        file_text = extract_text_from_file(file_obj)\n",
        "        suggestions = generate_prompt_suggestions(file_text)\n",
        "        prompt_html = \"<div><b>Suggested Prompts:</b><br>\" + \"<br>\".join(f\"- {suggestion}\" for suggestion in suggestions) + \"</div>\"\n",
        "        return prompt_html\n",
        "\n",
        "    # Update chat window with user and assistant messages\n",
        "    def update_chat_window(messages):\n",
        "        chat_html = \"\"\n",
        "        for message in messages:\n",
        "            role = message[\"role\"]\n",
        "            content = message[\"content\"]\n",
        "            if role == \"user\":\n",
        "                chat_html += f\"<div class='message user-message'>{content}</div>\"\n",
        "            else:\n",
        "                chat_html += f\"<div class='message assistant-message'>{content}</div>\"\n",
        "        return chat_html\n",
        "\n",
        "    # Function for chatbot interaction\n",
        "    def interact(prompt, file_obj, history, max_new_tokens):\n",
        "        response, new_history = chatbot_fn(prompt, file_obj, history, max_new_tokens)\n",
        "        chat_html = update_chat_window(new_history)\n",
        "        return chat_html, new_history\n",
        "\n",
        "    # Bind the file upload to generate prompt suggestions\n",
        "    file_input.change(update_suggested_prompts, inputs=[file_input], outputs=[suggested_prompts])\n",
        "\n",
        "    # Bind the button to the chatbot function\n",
        "    submit_button.click(interact, [user_prompt, file_input, chatbot_history, max_new_tokens], [chatbox, chatbot_history])\n",
        "\n",
        "# Launch the Gradio interface with share=True to create a public link (use this for testing externally)\n",
        "iface.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "FxB67tVDCLoD",
        "outputId": "88a6ceb7-43fc-4877-8bc8-283405d5b58b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0f09b5d303d272c0f8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0f09b5d303d272c0f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://0f09b5d303d272c0f8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries in Google Colab\n",
        "!pip install transformers gradio pdfplumber torch pyngrok --quiet\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pdfplumber  # Optimized PDF processing\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Use a smaller model like DistilGPT-2 or GPT-2\n",
        "model_name = \"distilgpt2\"  # You can also use \"gpt2\" or \"flan-t5-small\"\n",
        "\n",
        "# Use GPU if available, else fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Cache for tokenized inputs (to avoid re-tokenizing the same content)\n",
        "token_cache = {}\n",
        "\n",
        "# Function to extract text from the PDF using pdfplumber for faster processing\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() if page.extract_text() else ''\n",
        "    return text.strip() if text else \"Error: Could not extract text from PDF.\"\n",
        "\n",
        "# Function to extract text from a .txt file\n",
        "def extract_text_from_txt(txt_file):\n",
        "    try:\n",
        "        text = txt_file.read().decode('utf-8')\n",
        "    except Exception as e:\n",
        "        return f\"Error: Could not read the text file. Details: {str(e)}\"\n",
        "\n",
        "    return text.strip() if text else \"Error: Text file is empty.\"\n",
        "\n",
        "# Load the file and extract its content based on the file content\n",
        "def extract_text_from_file(file_obj):\n",
        "    # Check if file_obj is None (no file uploaded)\n",
        "    if file_obj is None:\n",
        "        return \"Error: No file uploaded. Please upload a PDF or TXT file.\"\n",
        "\n",
        "    # Check if the file is a PDF or TXT file based on content type\n",
        "    file_name = file_obj.name if hasattr(file_obj, 'name') else \"unknown\"\n",
        "\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(file_obj)\n",
        "    elif file_name.endswith(\".txt\"):\n",
        "        return extract_text_from_txt(file_obj)\n",
        "    else:\n",
        "        return \"Error: Unsupported file format. Please provide a .pdf or .txt file.\"\n",
        "\n",
        "# Generate prompt suggestions based on content\n",
        "def generate_prompt_suggestions(file_text):\n",
        "    # Analyze the content to suggest some starter prompts (this is a basic implementation)\n",
        "    if \"chapter\" in file_text.lower() or \"introduction\" in file_text.lower():\n",
        "        return [\"Can you summarize this chapter?\", \"What is the key takeaway from the introduction?\", \"Explain the main argument.\"]\n",
        "    elif \"conclusion\" in file_text.lower() or \"results\" in file_text.lower():\n",
        "        return [\"What are the results mentioned in the document?\", \"Can you summarize the conclusion?\", \"What are the key findings?\"]\n",
        "    else:\n",
        "        return [\"What is this document about?\", \"Summarize the content of this file.\", \"Can you explain the key points?\"]\n",
        "\n",
        "# Define the chatbot function\n",
        "def chatbot_fn(prompt, file_obj, chatbot_history=None, max_new_tokens=150):\n",
        "    try:\n",
        "        # Initialize chatbot_history if it's None\n",
        "        if chatbot_history is None:\n",
        "            chatbot_history = []\n",
        "\n",
        "        # Extract text from the file\n",
        "        file_text = extract_text_from_file(file_obj)\n",
        "\n",
        "        if file_text.startswith(\"Error\"):\n",
        "            return file_text, chatbot_history  # Return the error message\n",
        "\n",
        "        # Cache tokenized inputs to avoid redundant work (faster tokenization)\n",
        "        if file_text not in token_cache:\n",
        "            token_cache[file_text] = tokenizer(file_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "        inputs = token_cache[file_text]  # Retrieve tokenized input from cache\n",
        "\n",
        "        # Generate response with `max_new_tokens`\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # Decode the response\n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Check if the \"Assistant:\" label exists in the response\n",
        "        if \"Assistant:\" in response_text:\n",
        "            assistant_response = response_text.split(\"Assistant:\")[-1].strip()\n",
        "        else:\n",
        "            assistant_response = response_text if response_text else \"I couldn't generate a proper response.\"\n",
        "\n",
        "        # Update chatbot history with the user and assistant responses\n",
        "        chatbot_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "        chatbot_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        return assistant_response, chatbot_history\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", chatbot_history\n",
        "\n",
        "# Simplified Gradio Interface (light mode, no custom CSS)\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"<h1>Document-based Chatbot (Light Mode)</h1>\")\n",
        "\n",
        "    # Chat window\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            chatbox = gr.HTML(elem_id=\"chatbox\", value=\"\")\n",
        "\n",
        "    # Inputs\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            user_prompt = gr.Textbox(label=\"Your message\")\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(label=\"Upload PDF or TXT file\")\n",
        "\n",
        "    # Slider for Max New Tokens\n",
        "    max_new_tokens = gr.Slider(50, 300, step=10, label=\"Max New Tokens\")\n",
        "\n",
        "    # Suggested prompts area\n",
        "    with gr.Row():\n",
        "        suggested_prompts = gr.HTML(\"\")\n",
        "\n",
        "    # Output area (chat history)\n",
        "    output_text = gr.Textbox(visible=False)\n",
        "\n",
        "    # Chat history state\n",
        "    chatbot_history = gr.State([])\n",
        "\n",
        "    # Button to submit\n",
        "    submit_button = gr.Button(\"Send\")\n",
        "\n",
        "    # Function to generate and display suggested prompts\n",
        "    def update_suggested_prompts(file_obj):\n",
        "        file_text = extract_text_from_file(file_obj)\n",
        "        suggestions = generate_prompt_suggestions(file_text)\n",
        "        prompt_html = \"<div><b>Suggested Prompts:</b><br>\" + \"<br>\".join(f\"- {suggestion}\" for suggestion in suggestions) + \"</div>\"\n",
        "        return prompt_html\n",
        "\n",
        "    # Update chat window with user and assistant messages\n",
        "    def update_chat_window(messages):\n",
        "        chat_html = \"\"\n",
        "        for message in messages:\n",
        "            role = message[\"role\"]\n",
        "            content = message[\"content\"]\n",
        "            if role == \"user\":\n",
        "                chat_html += f\"<div class='message user-message'>{content}</div>\"\n",
        "            else:\n",
        "                chat_html += f\"<div class='message assistant-message'>{content}</div>\"\n",
        "        return chat_html\n",
        "\n",
        "    # Function for chatbot interaction\n",
        "    def interact(prompt, file_obj, history, max_new_tokens):\n",
        "        response, new_history = chatbot_fn(prompt, file_obj, history, max_new_tokens)\n",
        "        chat_html = update_chat_window(new_history)\n",
        "        return chat_html, new_history\n",
        "\n",
        "    # Bind the file upload to generate prompt suggestions\n",
        "    file_input.change(update_suggested_prompts, inputs=[file_input], outputs=[suggested_prompts])\n",
        "\n",
        "    # Bind the button to the chatbot function\n",
        "    submit_button.click(interact, [user_prompt, file_input, chatbot_history, max_new_tokens], [chatbox, chatbot_history])\n",
        "\n",
        "# Start Gradio on a random port to avoid port conflicts\n",
        "port = random.randint(8000, 9000)  # Dynamic port selection\n",
        "iface.launch(server_name=\"0.0.0.0\", server_port=port, share=False)\n",
        "\n",
        "# Start Ngrok tunnel to expose the Gradio app\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"Ngrok Tunnel URL: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "u149-IPIDcmw",
        "outputId": "8c51ae01-bd88-4ffd-8bc0-37c5c16fa903"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(8915, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Tunnel URL: NgrokTunnel: \"https://7103-35-224-185-184.ngrok-free.app\" -> \"http://localhost:8915\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2nrVQ6aD9BlhbMim7so6JxInX3w_3zTa7muTpscFegJCi4s6Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxIAzxK4CoW3",
        "outputId": "a55bfd19-b604-403d-c45e-7b5ce456ab60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    }
  ]
}